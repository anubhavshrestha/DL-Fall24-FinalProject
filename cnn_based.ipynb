{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5af5a7-fb16-4003-972c-b6f7d6196f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels=2):\n",
    "        super().__init__()\n",
    "        # First conv: 65x65 -> 22x22\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=7, stride=3, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # Second conv: 22x22 -> 8x8\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=3, padding=2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Final conv: maintain 8x8 but process features\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.repr_dim = 32\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # -> 22x22\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # -> 8x8\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # -> 8x8\n",
    "        return x  # Output shape: [B, 32, 8, 8]\n",
    "\n",
    "class TransitionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Action embedding\n",
    "        self.action_embed = nn.Sequential(\n",
    "            nn.Conv2d(2, 16, 1),  # First go to intermediate dimension\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, hidden_dim, 1),  # Then to full hidden_dim\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Transition model\n",
    "        self.transition = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim * 2, hidden_dim, 3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: [B, hidden_dim, 8, 8] - Current state representation\n",
    "            action: [B, 2] - (dx, dy) action\n",
    "        \"\"\"\n",
    "        B, _, H, W = state.shape\n",
    "        \n",
    "        # Expand action to spatial dimensions and embed\n",
    "        action = action.view(B, 2, 1, 1).expand(-1, -1, H, W)\n",
    "        action_embedding = self.action_embed(action)\n",
    "        \n",
    "        # Combine state and action\n",
    "        combined = torch.cat([state, action_embedding], dim=1)\n",
    "        \n",
    "        # Predict state change\n",
    "        delta = self.transition(combined)\n",
    "        \n",
    "        # Residual connection\n",
    "        next_state = state + delta\n",
    "        \n",
    "        return next_state\n",
    "\n",
    "class WorldModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_channels=2)\n",
    "        self.predictor = TransitionModel(hidden_dim=32)\n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            states: [B, 1, 2, 65, 65] - Initial state only\n",
    "            actions: [B, T-1, 2] - Sequence of T-1 actions\n",
    "        Returns:\n",
    "            predictions: [B, T, 32, 8, 8] - Predicted representations\n",
    "        \"\"\"\n",
    "        B, _, _, H, W = states.shape\n",
    "        T = actions.shape[1] + 1\n",
    "        \n",
    "        # Get initial state encoding\n",
    "        curr_state = self.encoder(states.squeeze(1))\n",
    "        predictions = [curr_state]\n",
    "        \n",
    "        # Predict future states\n",
    "        for t in range(T-1):\n",
    "            curr_state = self.predictor(curr_state, actions[:, t])\n",
    "            predictions.append(curr_state)\n",
    "            \n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e954244-00c2-4c71-9ad6-18f5c58d7d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model tests with real data...\n",
      "\n",
      "Input Data Shapes:\n",
      "States shape: torch.Size([256, 17, 2, 65, 65])\n",
      "Actions shape: torch.Size([256, 16, 2])\n",
      "States min/max: 0.00, 0.09\n",
      "Actions min/max: -1.76, 1.76\n",
      "\n",
      "Encoder output shape: torch.Size([256, 32, 8, 8])\n",
      "\n",
      "Single action sample:\n",
      "Action values: tensor([-0.4032, -1.0003], device='cuda:0')\n",
      "\n",
      "Action processing:\n",
      "Action after spatial expansion: torch.Size([256, 2, 8, 8])\n",
      "Action after embedding: torch.Size([256, 32, 8, 8])\n",
      "\n",
      "Prediction shapes:\n",
      "Single step prediction: torch.Size([256, 32, 8, 8])\n",
      "Full sequence prediction: torch.Size([256, 17, 32, 8, 8])\n",
      "\n",
      "Compression Statistics:\n",
      "Input size per frame: 8450 values\n",
      "Encoded size per frame: 2048 values\n",
      "Compression ratio: 4.13x\n",
      "\n",
      "All dimension tests passed!\n",
      "\n",
      "Testing multiple forward passes...\n",
      "Batch 1 processed successfully.\n",
      "Prediction stats for batch 1:\n",
      "Mean: 0.394\n",
      "Std: 7.925\n",
      "Min: -77.734\n",
      "Max: 99.804\n",
      "Batch 2 processed successfully.\n",
      "Batch 3 processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataset import create_wall_dataloader\n",
    "\n",
    "def test_model_with_real_data():\n",
    "    # Initialize models with new dimensions\n",
    "    encoder = Encoder(input_channels=2).to('cuda')\n",
    "    predictor = TransitionModel(hidden_dim=32).to('cuda')\n",
    "    world_model = WorldModel().to('cuda')\n",
    "    \n",
    "    # Create data loader with actual data\n",
    "    data_path = \"/scratch/an3854/DL24FA/train\"\n",
    "    dataloader = create_wall_dataloader(\n",
    "        data_path=data_path,\n",
    "        probing=False,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        batch_size=256,\n",
    "        train=True\n",
    "    )\n",
    "    \n",
    "    # Get a batch of data\n",
    "    batch = next(iter(dataloader))\n",
    "    states = batch.states  # [B, T, C, H, W]\n",
    "    actions = batch.actions  # [B, T-1, 2]\n",
    "    \n",
    "    print(\"\\nInput Data Shapes:\")\n",
    "    print(f\"States shape: {states.shape}\")  # Should be [4, T, 2, 65, 65]\n",
    "    print(f\"Actions shape: {actions.shape}\")  # Should be [4, T-1, 2]\n",
    "    print(f\"States min/max: {states.min():.2f}, {states.max():.2f}\")\n",
    "    print(f\"Actions min/max: {actions.min():.2f}, {actions.max():.2f}\")\n",
    "    \n",
    "    # Test encoder with initial state\n",
    "    init_state = states[:, 0]  # Take first timestep\n",
    "    encoded = encoder(init_state)\n",
    "    print(f\"\\nEncoder output shape: {encoded.shape}\")  # Should be [4, 32, 8, 8]\n",
    "    \n",
    "    # Test predictor with a single action\n",
    "    single_action = actions[:, 0]  # Take first action\n",
    "    print(f\"\\nSingle action sample:\")\n",
    "    print(f\"Action values: {single_action[0]}\")  # Print first batch's action\n",
    "    \n",
    "    # Test action processing\n",
    "    B, _, h, w = encoded.shape\n",
    "    action_spatial = single_action.view(B, 2, 1, 1).expand(-1, -1, h, w)\n",
    "    print(f\"\\nAction processing:\")\n",
    "    print(f\"Action after spatial expansion: {action_spatial.shape}\")  # Should be [4, 2, 8, 8]\n",
    "    \n",
    "    # Test action embedding\n",
    "    action_embedded = predictor.action_embed(action_spatial)\n",
    "    print(f\"Action after embedding: {action_embedded.shape}\")  # Should be [4, 32, 8, 8]\n",
    "    \n",
    "    # Test single prediction step\n",
    "    next_state = predictor(encoded, single_action)\n",
    "    print(f\"\\nPrediction shapes:\")\n",
    "    print(f\"Single step prediction: {next_state.shape}\")  # Should be [4, 32, 8, 8]\n",
    "    \n",
    "    # Test full sequence prediction\n",
    "    init_states = states[:, 0:1]  # [B, 1, C, H, W]\n",
    "    sequence_actions = actions  # [B, T-1, 2]\n",
    "    predictions = world_model(init_states, sequence_actions)\n",
    "    print(f\"Full sequence prediction: {predictions.shape}\")\n",
    "    # Should be [B, T, 32, 8, 8]\n",
    "    \n",
    "    # Print compression stats\n",
    "    input_size = states.shape[-2] * states.shape[-1] * states.shape[-3]\n",
    "    encoded_size = encoded.shape[-2] * encoded.shape[-1] * encoded.shape[-3]\n",
    "    compression_ratio = input_size / encoded_size\n",
    "    print(f\"\\nCompression Statistics:\")\n",
    "    print(f\"Input size per frame: {input_size} values\")\n",
    "    print(f\"Encoded size per frame: {encoded_size} values\")\n",
    "    print(f\"Compression ratio: {compression_ratio:.2f}x\")\n",
    "    \n",
    "    # Verify shapes\n",
    "    assert encoded.shape[1] == 32, \"Channel dimension mismatch\"\n",
    "    assert encoded.shape[2] == encoded.shape[3] == 8, \"Spatial dimensions mismatch\"\n",
    "    assert predictions.shape[1] == actions.shape[1] + 1, \"Sequence length mismatch\"\n",
    "    assert predictions.shape[2] == 32, \"Output channel mismatch\"\n",
    "    assert predictions.shape[3] == predictions.shape[4] == 8, \"Output spatial dimension mismatch\"\n",
    "    \n",
    "    print(\"\\nAll dimension tests passed!\")\n",
    "    \n",
    "    # Test multiple forward passes\n",
    "    print(\"\\nTesting multiple forward passes...\")\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= 3:  # Test with 3 batches\n",
    "            break\n",
    "        states = batch.states\n",
    "        actions = batch.actions\n",
    "        init_states = states[:, 0:1]\n",
    "        with torch.no_grad():\n",
    "            predictions = world_model(init_states, actions)\n",
    "        print(f\"Batch {i+1} processed successfully.\")\n",
    "        \n",
    "        # Print some statistics about the predictions\n",
    "        if i == 0:\n",
    "            print(f\"Prediction stats for batch {i+1}:\")\n",
    "            print(f\"Mean: {predictions.mean():.3f}\")\n",
    "            print(f\"Std: {predictions.std():.3f}\")\n",
    "            print(f\"Min: {predictions.min():.3f}\")\n",
    "            print(f\"Max: {predictions.max():.3f}\")\n",
    "    \n",
    "    return world_model, dataloader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running model tests with real data...\")\n",
    "    model, dataloader = test_model_with_real_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757cde1-d5d0-4a18-8c98-384222893a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from dataset import WallSample\n",
    "\n",
    "class VICRegLoss(nn.Module):\n",
    "    def __init__(self, lambda_param=25.0, mu_param=25.0, nu_param=1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_param = lambda_param  # invariance loss coefficient\n",
    "        self.mu_param = mu_param         # variance loss coefficient\n",
    "        self.nu_param = nu_param         # covariance loss coefficient\n",
    "    \n",
    "    def off_diagonal(self, x):\n",
    "        \"\"\"Return off-diagonal elements of a square matrix\"\"\"\n",
    "        n = x.shape[0]\n",
    "        return x.flatten()[:-1].view(n-1, n+1)[:, 1:].flatten()\n",
    "    \n",
    "    def forward(self, z_a, z_b):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z_a, z_b: Batch of representations [N, D]\n",
    "        Returns:\n",
    "            total_loss: Combined VICReg loss\n",
    "            losses: Dictionary containing individual loss components\n",
    "        \"\"\"\n",
    "        N = z_a.shape[0]  # batch size\n",
    "        D = z_a.shape[1]  # dimension\n",
    "        \n",
    "        # Invariance loss (MSE)\n",
    "        sim_loss = F.mse_loss(z_a, z_b)\n",
    "        \n",
    "        # Variance loss\n",
    "        std_z_a = torch.sqrt(z_a.var(dim=0) + 1e-04)\n",
    "        std_z_b = torch.sqrt(z_b.var(dim=0) + 1e-04)\n",
    "        std_loss = (torch.mean(F.relu(1 - std_z_a)) + \n",
    "                   torch.mean(F.relu(1 - std_z_b)))\n",
    "        \n",
    "        # Covariance loss\n",
    "        z_a = z_a - z_a.mean(dim=0)\n",
    "        z_b = z_b - z_b.mean(dim=0)\n",
    "        \n",
    "        cov_z_a = (z_a.T @ z_a) / (N - 1)\n",
    "        cov_z_b = (z_b.T @ z_b) / (N - 1)\n",
    "        \n",
    "        cov_loss = (self.off_diagonal(cov_z_a).pow_(2).sum() / D +\n",
    "                   self.off_diagonal(cov_z_b).pow_(2).sum() / D)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = (self.lambda_param * sim_loss + \n",
    "                     self.mu_param * std_loss + \n",
    "                     self.nu_param * cov_loss)\n",
    "        \n",
    "        # Return individual losses for logging\n",
    "        losses = {\n",
    "            'sim_loss': sim_loss.item(),\n",
    "            'std_loss': std_loss.item(),\n",
    "            'cov_loss': cov_loss.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "        \n",
    "        return total_loss, losses\n",
    "\n",
    "class WorldModelVICReg(nn.Module):\n",
    "    def __init__(self, lambda_param=25.0, mu_param=25.0, nu_param=1.0):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_channels=2)\n",
    "        self.predictor = TransitionModel(hidden_dim=32)\n",
    "        self.criterion = VICRegLoss(lambda_param, mu_param, nu_param)\n",
    "    \n",
    "    def compute_vicreg_loss(self, pred_state, target_obs):\n",
    "        \"\"\"Compute VICReg loss between predicted and encoded target states\"\"\"\n",
    "        # Get target encoding\n",
    "        target_state = self.encoder(target_obs)\n",
    "        \n",
    "        # Flatten spatial dimensions: [B, 32, 8, 8] -> [B, 2048]\n",
    "        pred_flat = pred_state.flatten(start_dim=1)\n",
    "        target_flat = target_state.flatten(start_dim=1)\n",
    "        \n",
    "        # Compute VICReg losses\n",
    "        total_loss, component_losses = self.criterion(pred_flat, target_flat)\n",
    "        return total_loss, component_losses\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        states = batch.states\n",
    "        actions = batch.actions\n",
    "        \n",
    "        # Get initial state\n",
    "        init_states = states[:, 0:1]\n",
    "        \n",
    "        # Get predictions for all steps\n",
    "        predictions = self.forward_prediction(init_states, actions)\n",
    "        \n",
    "        # Initialize losses\n",
    "        total_loss = 0.0\n",
    "        accumulated_losses = {\n",
    "            'sim_loss': 0.0,\n",
    "            'std_loss': 0.0,\n",
    "            'cov_loss': 0.0,\n",
    "            'total_loss': 0.0\n",
    "        }\n",
    "        \n",
    "        # Compute VICReg loss for each timestep\n",
    "        for t in range(actions.shape[1]):\n",
    "            pred_state = predictions[:, t+1]\n",
    "            target_obs = states[:, t+1]\n",
    "            \n",
    "            loss, component_losses = self.compute_vicreg_loss(pred_state, target_obs)\n",
    "            total_loss += loss\n",
    "            \n",
    "            # Accumulate component losses\n",
    "            for k in accumulated_losses:\n",
    "                accumulated_losses[k] += component_losses[k]\n",
    "        \n",
    "        # Average losses over timesteps\n",
    "        for k in accumulated_losses:\n",
    "            accumulated_losses[k] /= actions.shape[1]\n",
    "        \n",
    "        return total_loss / actions.shape[1], predictions, accumulated_losses\n",
    "\n",
    "\n",
    "    def forward_prediction(self, states, actions):\n",
    "        \"\"\"\n",
    "        Forward pass for prediction of future states\n",
    "        Args:\n",
    "            states: [B, 1, 2, 65, 65] - Initial state only\n",
    "            actions: [B, T-1, 2] - Sequence of T-1 actions\n",
    "        Returns:\n",
    "            predictions: [B, T, 32, 8, 8] - Predicted representations\n",
    "        \"\"\"\n",
    "        B, _, _, H, W = states.shape\n",
    "        T = actions.shape[1] + 1\n",
    "        \n",
    "        # Get initial state encoding\n",
    "        curr_state = self.encoder(states.squeeze(1))\n",
    "        predictions = [curr_state]\n",
    "        \n",
    "        # Predict future states\n",
    "        for t in range(T-1):\n",
    "            curr_state = self.predictor(curr_state, actions[:, t])\n",
    "            predictions.append(curr_state)\n",
    "            \n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb94ac6-04ce-449e-9ff8-530b0c297049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldModelTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, learning_rate=1e-3, \n",
    "                 device='cuda', log_dir='runs'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Initialize tensorboard writer\n",
    "        current_time = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "        self.writer = SummaryWriter(f'{log_dir}/{current_time}')\n",
    "        \n",
    "        # Save hyperparameters\n",
    "        self.writer.add_text('hyperparameters', f'''\n",
    "        learning_rate: {learning_rate}\n",
    "        batch_size: {train_loader.batch_size}\n",
    "        model_channels: {model.encoder.repr_dim}\n",
    "        ''')\n",
    "\n",
    "    def validate(self, epoch):\n",
    "        self.model.eval()\n",
    "        total_losses = {\n",
    "            'total_loss': 0.0,\n",
    "            'sim_loss': 0.0,\n",
    "            'std_loss': 0.0,\n",
    "            'cov_loss': 0.0\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                # Move batch to device without modifying the tuple\n",
    "                states = batch.states.to(self.device)\n",
    "                actions = batch.actions.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                _, _, component_losses = self.model.training_step(\n",
    "                    WallSample(states=states, actions=actions, locations=batch.locations)\n",
    "                )\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_losses = {\n",
    "            'total_loss': 0.0,\n",
    "            'sim_loss': 0.0,\n",
    "            'std_loss': 0.0,\n",
    "            'cov_loss': 0.0\n",
    "        }\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(self.train_loader, desc=f\"Epoch {epoch}\")):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Move batch to device without modifying the tuple\n",
    "            states = batch.states.to(self.device)\n",
    "            actions = batch.actions.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            loss, predictions, component_losses = self.model.training_step(\n",
    "                WallSample(states=states, actions=actions, locations=batch.locations)\n",
    "            )\n",
    "\n",
    "    \n",
    "    def save_checkpoint(self, epoch, loss):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }\n",
    "        path = f'checkpoints/checkpoint_epoch_{epoch}.pt'\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        torch.save(checkpoint, path)\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"Complete training loop\"\"\"\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_losses = self.train_epoch(epoch)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss = self.validate(epoch)\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(\"\\nEpoch Summary:\")\n",
    "            print(\"Training Losses:\")\n",
    "            for k, v in train_losses.items():\n",
    "                print(f\"{k}: {v:.4f}\")\n",
    "            print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.save_checkpoint(epoch, val_loss)\n",
    "                print(\"New best model saved!\")\n",
    "            \n",
    "            # Regular checkpoint every 5 epochs\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_checkpoint(epoch, val_loss)\n",
    "                print(f\"Regular checkpoint saved at epoch {epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9ae01-d991-481b-b466-8352253dbbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with custom coefficients\n",
    "model = WorldModelVICReg(\n",
    "    lambda_param=25.0,  # invariance loss coefficient\n",
    "    mu_param=25.0,      # variance loss coefficient\n",
    "    nu_param=1.0        # covariance loss coefficient\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = create_wall_dataloader(\n",
    "    data_path=\"/scratch/an3854/DL24FA/train\",\n",
    "    probing=False,\n",
    "    device=\"cuda\",\n",
    "    batch_size=128,\n",
    "    train=True,  \n",
    "    num_samples=10000\n",
    ")\n",
    "\n",
    "val_loader = create_wall_dataloader(\n",
    "    data_path=\"/scratch/an3854/DL24FA/train\",\n",
    "    probing=False,\n",
    "    device=\"cuda\",\n",
    "    batch_size=128,\n",
    "    train=False, \n",
    "    num_samples=2000\n",
    ")\n",
    "\n",
    "# Initialize trainer and train\n",
    "trainer = WorldModelTrainer(model, train_loader, val_loader)\n",
    "trainer.train(num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9bd16-8568-4df4-ae39-f0684b2ba38e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aadim)",
   "language": "python",
   "name": "aadim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
