{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from typing import Tuple, Dict\n",
    "from dataset import WallSample, create_wall_dataloader\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from lightly.utils.scheduler import cosine_schedule\n",
    "\n",
    "from evaluator import ProbingEvaluator\n",
    "\n",
    "def load_data(device):\n",
    "    data_path = \"/drive_reader/as16386/DL24FA\"\n",
    "\n",
    "    probe_train_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/probe_normal/train\",\n",
    "        probing=True,\n",
    "        device=device,\n",
    "        train=True,\n",
    "    )\n",
    "\n",
    "    probe_val_normal_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/probe_normal/val\",\n",
    "        probing=True,\n",
    "        device=device,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    probe_val_wall_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/probe_wall/val\",\n",
    "        probing=True,\n",
    "        device=device,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    probe_val_ds = {\"normal\": probe_val_normal_ds, \"wall\": probe_val_wall_ds}\n",
    "\n",
    "    return probe_train_ds, probe_val_ds\n",
    "\n",
    "def evaluate_model(device, model, probe_train_ds, probe_val_ds):\n",
    "    evaluator = ProbingEvaluator(\n",
    "        device=device,\n",
    "        model=model,\n",
    "        probe_train_ds=probe_train_ds,\n",
    "        probe_val_ds=probe_val_ds,\n",
    "        quick_debug=False,\n",
    "    )\n",
    "\n",
    "    prober = evaluator.train_pred_prober()\n",
    "\n",
    "    avg_losses = evaluator.evaluate_all(prober=prober)\n",
    "\n",
    "    for probe_attr, loss in avg_losses.items():\n",
    "        print(f\"{probe_attr} loss: {loss}\")\n",
    "\n",
    "probe_train_ds, probe_val_ds = load_data(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2024-12-11 23:55:14\n",
      "Training for 100 epochs\n",
      "Checkpoints will be saved to checkpoints/combined_barlow_vicreg_normalized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/100: 100%|██████████| 1148/1148 [02:20<00:00,  8.18it/s, loss=0.9949, time/batch=0.077s, gpu_mem=1.4GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0/100 Summary:\n",
      "Train Loss: 0.9949\n",
      "Component Losses:\n",
      "  total_loss: 0.9949\n",
      "  vicreg_sim_loss: 24.1968\n",
      "  vicreg_std_loss: 1.1923\n",
      "  vicreg_cov_loss: 4545.7000\n",
      "  barlow_loss: 1.0009\n",
      "Epoch Time: 140.3s\n",
      "Avg Batch Time: 0.077s\n",
      "GPU Memory: 1353MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Probe prediction epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized pred locations loss 1.1327272653579712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized pred locations loss 0.33933916687965393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Probe prediction step: 100%|██████████| 156/156 [00:08<00:00, 17.43it/s]\n",
      "Probe prediction epochs:   5%|▌         | 1/20 [00:08<02:50,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized pred locations loss 0.16049417853355408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized pred locations loss 0.08976524323225021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Probe prediction step: 100%|██████████| 156/156 [00:08<00:00, 17.63it/s]\n",
      "Probe prediction epochs:  10%|█         | 2/20 [00:17<02:40,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized pred locations loss 0.06290233880281448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Probe prediction step: 100%|██████████| 156/156 [00:16<00:00,  9.72it/s]\n",
      "Probe prediction epochs:  15%|█▌        | 3/20 [00:33<03:26, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized pred locations loss 0.047875333577394485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Probe prediction step:  31%|███       | 48/156 [00:05<00:11,  9.29it/s]\n",
      "Probe prediction epochs:  15%|█▌        | 3/20 [00:39<03:41, 13.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 366\u001b[0m\n\u001b[1;32m    359\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m create_wall_dataloader(\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/drive_reader/as16386/DL24FA/train\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Update with your path\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m    362\u001b[0m     train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    363\u001b[0m )\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoints/combined_barlow_vicreg_normalized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m    373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 342\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, num_epochs, learning_rate, save_dir, save_frequency)\u001b[0m\n\u001b[1;32m    334\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss,\n\u001b[1;32m    339\u001b[0m     }, checkpoint_path)\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 342\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobe_train_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobe_val_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(device, model, probe_train_ds, probe_val_ds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(device, model, probe_train_ds, probe_val_ds):\n\u001b[1;32m     44\u001b[0m     evaluator \u001b[38;5;241m=\u001b[39m ProbingEvaluator(\n\u001b[1;32m     45\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     46\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m         quick_debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 52\u001b[0m     prober \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_pred_prober\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     avg_losses \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate_all(prober\u001b[38;5;241m=\u001b[39mprober)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m probe_attr, loss \u001b[38;5;129;01min\u001b[39;00m avg_losses\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/Chill-Pill/evaluator.py:114\u001b[0m, in \u001b[0;36mProbingEvaluator.train_pred_prober\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m Scheduler(\n\u001b[1;32m    104\u001b[0m     schedule\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mschedule,\n\u001b[1;32m    105\u001b[0m     base_lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbe prediction epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbe prediction step\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;66;03m# TODO: Forward pass through your model\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         pred_encs \u001b[38;5;241m=\u001b[39m model(states\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mstates, actions\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mactions)\n\u001b[1;32m    118\u001b[0m         pred_encs \u001b[38;5;241m=\u001b[39m pred_encs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# # BS, T, D --> T, BS, D\u001b[39;00m\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/Chill-Pill/dataset.py:32\u001b[0m, in \u001b[0;36mWallDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[1;32m     31\u001b[0m     states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates[i])\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 32\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         locations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocations[i])\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from typing import Tuple, Dict\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dataset import create_wall_dataloader, WallSample\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 8, kernel_size=5, stride=3, padding=2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8, 32, kernel_size=3, stride=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.repr_dim = 32 * 8 * 8\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "class TransitionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.action_embed = nn.Sequential(\n",
    "            nn.Conv2d(2, hidden_dim // 2, 1),\n",
    "            nn.BatchNorm2d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim // 2, hidden_dim, 1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.transition = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim * 2, hidden_dim, 3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        B, _, H, W = state.shape\n",
    "        action = action.view(B, 2, 1, 1).expand(-1, -1, H, W)\n",
    "        action_embedding = self.action_embed(action)\n",
    "        combined = torch.cat([state, action_embedding], dim=1)\n",
    "        delta = self.transition(combined)\n",
    "        next_state = state + delta\n",
    "        return next_state\n",
    "\n",
    "class NormalizedCombinedLoss(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vicreg_sim_coef=25.0,\n",
    "                 vicreg_std_coef=25.0,\n",
    "                 vicreg_cov_coef=1.0,\n",
    "                 barlow_lambda=0.005,\n",
    "                 loss_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.vicreg_sim_coef = vicreg_sim_coef\n",
    "        self.vicreg_std_coef = vicreg_std_coef\n",
    "        self.vicreg_cov_coef = vicreg_cov_coef\n",
    "        self.barlow_lambda = barlow_lambda\n",
    "        self.loss_weight = loss_weight\n",
    "        \n",
    "        # Running statistics for normalization\n",
    "        self.register_buffer('vicreg_mean', torch.tensor(0.0))\n",
    "        self.register_buffer('barlow_mean', torch.tensor(0.0))\n",
    "        self.register_buffer('count', torch.tensor(0))\n",
    "        self.momentum = 0.9\n",
    "    \n",
    "    def off_diagonal(self, x):\n",
    "        n = x.shape[0]\n",
    "        return x.flatten()[:-1].view(n-1, n+1)[:, 1:].flatten()\n",
    "    \n",
    "    def update_means(self, vicreg_loss, barlow_loss):\n",
    "        with torch.no_grad():\n",
    "            if self.count == 0:\n",
    "                self.vicreg_mean = vicreg_loss.detach()\n",
    "                self.barlow_mean = barlow_loss.detach()\n",
    "            else:\n",
    "                self.vicreg_mean = self.momentum * self.vicreg_mean + (1 - self.momentum) * vicreg_loss.detach()\n",
    "                self.barlow_mean = self.momentum * self.barlow_mean + (1 - self.momentum) * barlow_loss.detach()\n",
    "            self.count += 1\n",
    "    \n",
    "    def forward(self, z_a, z_b):\n",
    "        N = z_a.shape[0]\n",
    "        D = z_a.shape[1]\n",
    "        \n",
    "        # VICReg components\n",
    "        sim_loss = F.mse_loss(z_a, z_b)\n",
    "        \n",
    "        std_z_a = torch.sqrt(z_a.var(dim=0) + 1e-04)\n",
    "        std_z_b = torch.sqrt(z_b.var(dim=0) + 1e-04)\n",
    "        std_loss = torch.mean(F.relu(1 - std_z_a)) + torch.mean(F.relu(1 - std_z_b))\n",
    "        \n",
    "        z_a_c = z_a - z_a.mean(dim=0)\n",
    "        z_b_c = z_b - z_b.mean(dim=0)\n",
    "        \n",
    "        cov_z_a = (z_a_c.T @ z_a_c) / (N - 1)\n",
    "        cov_z_b = (z_b_c.T @ z_b_c) / (N - 1)\n",
    "        \n",
    "        vicreg_cov_loss = (self.off_diagonal(cov_z_a).pow_(2).sum() / D +\n",
    "                          self.off_diagonal(cov_z_b).pow_(2).sum() / D)\n",
    "        \n",
    "        # Compute full VICReg loss\n",
    "        vicreg_loss = (self.vicreg_sim_coef * sim_loss +\n",
    "                      self.vicreg_std_coef * std_loss +\n",
    "                      self.vicreg_cov_coef * vicreg_cov_loss)\n",
    "        \n",
    "        # Barlow Twins components\n",
    "        z_a_norm = (z_a - z_a.mean(dim=0)) / (z_a.std(dim=0) + 1e-6)\n",
    "        z_b_norm = (z_b - z_b.mean(dim=0)) / (z_b.std(dim=0) + 1e-6)\n",
    "        \n",
    "        c = torch.mm(z_a_norm.T, z_b_norm) / N\n",
    "        \n",
    "        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n",
    "        off_diag = torch.triu(c.pow(2), diagonal=1).sum() + torch.tril(c.pow(2), diagonal=-1).sum()\n",
    "        barlow_loss = on_diag + self.barlow_lambda * off_diag\n",
    "        \n",
    "        # Update running means\n",
    "        self.update_means(vicreg_loss, barlow_loss)\n",
    "        \n",
    "        # Normalize losses using running means\n",
    "        eps = 1e-6\n",
    "        if self.count > 0:\n",
    "            vicreg_loss_norm = vicreg_loss / (self.vicreg_mean + eps)\n",
    "            barlow_loss_norm = barlow_loss / (self.barlow_mean + eps)\n",
    "        else:\n",
    "            vicreg_loss_norm = vicreg_loss\n",
    "            barlow_loss_norm = barlow_loss\n",
    "        \n",
    "        # Combine normalized losses\n",
    "        total_loss = self.loss_weight * vicreg_loss_norm + (1 - self.loss_weight) * barlow_loss_norm\n",
    "        \n",
    "        component_losses = {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'vicreg_loss': vicreg_loss.item(),\n",
    "            'vicreg_norm': vicreg_loss_norm.item(),\n",
    "            'barlow_loss': barlow_loss.item(),\n",
    "            'barlow_norm': barlow_loss_norm.item(),\n",
    "            'vicreg_mean': self.vicreg_mean.item(),\n",
    "            'barlow_mean': self.barlow_mean.item()\n",
    "        }\n",
    "        \n",
    "        return total_loss, component_losses\n",
    "\n",
    "class WorldModelCombined(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_channels=2)\n",
    "        self.predictor = TransitionModel(hidden_dim=32)\n",
    "        self.criterion = NormalizedCombinedLoss()\n",
    "        self.repr_dim = self.encoder.repr_dim\n",
    "    \n",
    "    def forward_prediction(self, states, actions):\n",
    "        B, _, _, H, W = states.shape\n",
    "        T = actions.shape[1] + 1\n",
    "        \n",
    "        curr_state = self.encoder(states.squeeze(1))\n",
    "        predictions = [curr_state]\n",
    "        \n",
    "        for t in range(T-1):\n",
    "            curr_state = self.predictor(curr_state, actions[:, t])\n",
    "            predictions.append(curr_state)\n",
    "            \n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        return predictions\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        init_states = states[:, 0:1]\n",
    "        predictions = self.forward_prediction(init_states, actions)\n",
    "        B, T, C, H, W = predictions.shape\n",
    "        predictions = predictions.view(B, T, -1)\n",
    "        return predictions\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        states = batch.states\n",
    "        actions = batch.actions\n",
    "        \n",
    "        init_states = states[:, 0:1]\n",
    "        predictions = self.forward_prediction(init_states, actions)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        accumulated_losses = {\n",
    "            'total_loss': 0.0,\n",
    "            'vicreg_loss': 0.0,\n",
    "            'vicreg_norm': 0.0,\n",
    "            'barlow_loss': 0.0,\n",
    "            'barlow_norm': 0.0\n",
    "        }\n",
    "        \n",
    "        for t in range(actions.shape[1]):\n",
    "            pred_state = predictions[:, t+1]\n",
    "            target_obs = states[:, t+1]\n",
    "            \n",
    "            target_state = self.encoder(target_obs)\n",
    "            \n",
    "            pred_flat = pred_state.flatten(start_dim=1)\n",
    "            target_flat = target_state.flatten(start_dim=1)\n",
    "            \n",
    "            loss, component_losses = self.criterion(pred_flat, target_flat)\n",
    "            \n",
    "            total_loss += loss\n",
    "            for k in accumulated_losses:\n",
    "                accumulated_losses[k] += component_losses[k]\n",
    "        \n",
    "        total_loss = total_loss / actions.shape[1]\n",
    "        for k in accumulated_losses:\n",
    "            accumulated_losses[k] /= actions.shape[1]\n",
    "        \n",
    "        return total_loss, predictions, accumulated_losses\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=100, learning_rate=3e-4, \n",
    "                save_dir='checkpoints/combined_normalized', save_frequency=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Starting training at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Training for {num_epochs} epochs\")\n",
    "    print(f\"Checkpoints will be saved to {save_dir}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(train_loader), \n",
    "                          total=len(train_loader),\n",
    "                          desc=f'Epoch {epoch}/{num_epochs}')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in progress_bar:\n",
    "            # Move batch to GPU\n",
    "            batch = batch._replace(\n",
    "                states=batch.states.cuda(),\n",
    "                actions=batch.actions.cuda(),\n",
    "                locations=batch.locations.cuda() if batch.locations is not None else None\n",
    "            )\n",
    "            \n",
    "            # Forward pass and compute loss\n",
    "            optimizer.zero_grad()\n",
    "            loss, _, component_losses = model.training_step(batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            total_loss += loss.item()\n",
    "            current_loss = total_loss / (batch_idx + 1)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{current_loss:.4f}',\n",
    "                'vicreg_mean': f'{model.criterion.vicreg_mean.item():.4f}',\n",
    "                'barlow_mean': f'{model.criterion.barlow_mean.item():.4f}'\n",
    "            })\n",
    "            \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % save_frequency == 0:\n",
    "            checkpoint_path = save_dir / f\"checkpoint_epoch_{epoch+1}.pt\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': current_loss,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "        \n",
    "        evaluate_model(\"cuda\", model, probe_train_ds, probe_val_ds)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch} Summary:\")\n",
    "        print(f\"Loss: {current_loss:.4f}\")\n",
    "        print(f\"VICReg Mean: {model.criterion.vicreg_mean.item():.4f}\")\n",
    "        print(f\"Barlow Mean: {model.criterion.barlow_mean.item():.4f}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model and data loader\n",
    "    model = WorldModelCombined().cuda()\n",
    "    \n",
    "    train_loader = create_wall_dataloader(\n",
    "        \"/drive_reader/as16386/DL24FA/train\",  # Update with your path\n",
    "        batch_size=32,\n",
    "        train=True\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        num_epochs=100,\n",
    "        learning_rate=3e-4,\n",
    "        save_dir='checkpoints/combined_barlow_vicreg_normalized',\n",
    "        save_frequency=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
