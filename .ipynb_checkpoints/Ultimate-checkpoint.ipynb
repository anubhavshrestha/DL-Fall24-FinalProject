{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91288f36-61a1-4056-9040-62ba4daed064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4594 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 241\u001b[0m\n\u001b[1;32m    239\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m NUM_EPOCHS\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 241\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Optionally save the model after training\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 217\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, epoch, total_epochs)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[1;32m    216\u001b[0m     cs \u001b[38;5;241m=\u001b[39m current_states[:, t]  \u001b[38;5;66;03m# [B, C, H, W]\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     ns \u001b[38;5;241m=\u001b[39m \u001b[43mnext_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m     \u001b[38;5;66;03m# [B, C, H, W]\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     a \u001b[38;5;241m=\u001b[39m actions[:, t]          \u001b[38;5;66;03m# [B, action_dim]\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     p1, z3 \u001b[38;5;241m=\u001b[39m model(cs, ns, a)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from lightly.models.utils import deactivate_requires_grad\n",
    "from lightly.utils.scheduler import cosine_schedule\n",
    "from dataset import create_wall_dataloader\n",
    "\n",
    "# Configurations\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 32\n",
    "STATE_CHANNELS = 2\n",
    "STATE_H = 65\n",
    "STATE_W = 65\n",
    "ACTION_DIM = 2\n",
    "SPATIAL_DIM = 8        # Spatial dimension for latent features\n",
    "OUTPUT_CHANNELS = 64    # Output channels for latent representation\n",
    "PROJ_DIM = 256\n",
    "INIT_MOMENTUM = 0.996\n",
    "LR = 3e-4\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Dataloader creation (assuming create_wall_dataloader returns a loader with (current_states, next_states, actions)\n",
    "train_loader = create_wall_dataloader(\n",
    "    \"/scratch/an3854/DL24FA/train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train=True, \n",
    ")\n",
    "\n",
    "class SpatialStateEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes an input state into a spatial feature map of size [C, spatial_dim, spatial_dim].\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=2, output_channels=64, spatial_dim=8):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # After these layers: 65x65 -> ~4x4\n",
    "        # Map 4x4x256 -> 4x4x64 and then upsample to 8x8 if needed\n",
    "        self.spatial_predictor = nn.Sequential(\n",
    "            nn.Conv2d(256, output_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(size=(spatial_dim, spatial_dim), mode='bilinear', align_corners=False) \n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)  \n",
    "        x = self.spatial_predictor(x)  \n",
    "        return x\n",
    "\n",
    "class ConvDetTransition(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional deterministic transition model:\n",
    "    Given current spatial latent and action, predicts next spatial latent.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_channels=64, action_dim=2, hidden_channels=128):\n",
    "        super().__init__()\n",
    "        self.action_proj = nn.Sequential(\n",
    "            nn.Linear(action_dim, hidden_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transition = nn.Sequential(\n",
    "            nn.Conv2d(state_channels + hidden_channels, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, state_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = state.shape\n",
    "        action_feat = self.action_proj(action)  # [B, hidden_channels]\n",
    "        action_feat = action_feat.unsqueeze(-1).unsqueeze(-1)  # [B, hidden_channels, 1, 1]\n",
    "        action_feat = action_feat.expand(B, action_feat.shape[1], H, W)  # broadcast over spatial dims\n",
    "        x = torch.cat([state, action_feat], dim=1)\n",
    "        next_state = self.transition(x)\n",
    "        return next_state\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    BYOL projection head:\n",
    "    Flattens spatial representation and projects to a lower-dimensional space.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=64, spatial_dim=8, proj_dim=256):\n",
    "        super().__init__()\n",
    "        input_dim = input_channels * spatial_dim * spatial_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, proj_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.flatten(start_dim=1)  # [B, C*H*W]\n",
    "        return self.net(x)\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    BYOL prediction head:\n",
    "    Takes the projection and predicts a representation closer to the target projection.\n",
    "    \"\"\"\n",
    "    def __init__(self, proj_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(proj_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, proj_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class BYOLConvDet(nn.Module):\n",
    "    \"\"\"\n",
    "    BYOL model using a convolutional deterministic state transition.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 state_channels=STATE_CHANNELS, \n",
    "                 spatial_dim=SPATIAL_DIM, \n",
    "                 output_channels=OUTPUT_CHANNELS, \n",
    "                 action_dim=ACTION_DIM, \n",
    "                 proj_dim=PROJ_DIM,\n",
    "                 momentum=INIT_MOMENTUM):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Online networks\n",
    "        self.online_encoder = SpatialStateEncoder(input_channels=state_channels, output_channels=output_channels, spatial_dim=spatial_dim)\n",
    "        self.online_transition = ConvDetTransition(state_channels=output_channels, action_dim=action_dim)\n",
    "        self.online_projection = ProjectionHead(input_channels=output_channels, spatial_dim=spatial_dim, proj_dim=proj_dim)\n",
    "        self.online_prediction = PredictionHead(proj_dim=proj_dim)\n",
    "        \n",
    "        # Target networks\n",
    "        self.target_encoder = copy.deepcopy(self.online_encoder)\n",
    "        self.target_projection = copy.deepcopy(self.online_projection)\n",
    "        deactivate_requires_grad(self.target_encoder)\n",
    "        deactivate_requires_grad(self.target_projection)\n",
    "        \n",
    "        self.momentum = momentum\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_target(self, m: float):\n",
    "        \"\"\"Momentum update for target networks.\"\"\"\n",
    "        for o, t in zip(self.online_encoder.parameters(), self.target_encoder.parameters()):\n",
    "            t.data = t.data * m + o.data * (1. - m)\n",
    "        for o, t in zip(self.online_projection.parameters(), self.target_projection.parameters()):\n",
    "            t.data = t.data * m + o.data * (1. - m)\n",
    "\n",
    "    def forward(self, current_state: torch.Tensor, next_state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Encode current state\n",
    "        z1 = self.online_encoder(current_state)  # [B, C, H, W]\n",
    "\n",
    "        # Predict next latent\n",
    "        z2_pred_spatial = self.online_transition(z1, action)  # [B, C, H, W]\n",
    "\n",
    "        # Online pipeline\n",
    "        z2_pred_proj = self.online_projection(z2_pred_spatial)  # [B, proj_dim]\n",
    "        p1 = self.online_prediction(z2_pred_proj)               # [B, proj_dim]\n",
    "\n",
    "        # Target pipeline (no grad)\n",
    "        with torch.no_grad():\n",
    "            z3_spatial = self.target_encoder(next_state)\n",
    "            z3_proj = self.target_projection(z3_spatial)  # [B, proj_dim]\n",
    "        \n",
    "        return p1, z3_proj\n",
    "\n",
    "def byol_loss(p: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "    p = F.normalize(p, dim=-1)\n",
    "    z = F.normalize(z, dim=-1)\n",
    "    return 2 - 2 * (p * z).sum(dim=-1).mean()\n",
    "\n",
    "model = BYOLConvDet().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def train_epoch(model: BYOLConvDet, \n",
    "                dataloader: DataLoader, \n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                epoch: int,\n",
    "                total_epochs: int) -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
    "        # batch = (current_states, next_states, actions)\n",
    "        # current_states: [B, T, C, H, W]\n",
    "        # next_states:    [B, T, C, H, W]\n",
    "        # actions:        [B, T, action_dim]\n",
    "        current_states, next_states, actions = batch\n",
    "        current_states = current_states.to(DEVICE)\n",
    "        next_states = next_states.to(DEVICE)\n",
    "        actions = actions.to(DEVICE)\n",
    "        \n",
    "        # Compute current momentum\n",
    "        m = cosine_schedule(epoch, total_epochs, INIT_MOMENTUM, 1.0)\n",
    "        \n",
    "        # We now loop over all time steps T and sum the losses.\n",
    "        B, T, C, H, W = current_states.shape\n",
    "        step_loss = 0.0\n",
    "        for t in range(T):\n",
    "            cs = current_states[:, t]  # [B, C, H, W]\n",
    "            ns = next_states[:, t]     # [B, C, H, W]\n",
    "            a = actions[:, t]          # [B, action_dim]\n",
    "            \n",
    "            p1, z3 = model(cs, ns, a)\n",
    "            l = byol_loss(p1, z3)\n",
    "            step_loss += l\n",
    "        \n",
    "        # Average loss over T steps\n",
    "        step_loss = step_loss / T\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        step_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update target networks\n",
    "        model.update_target(m)\n",
    "        \n",
    "        total_loss += step_loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = NUM_EPOCHS\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_epoch(model, train_loader, optimizer, epoch, num_epochs)\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Optionally save the model after training\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "print(\"Training complete. model_weights.pth saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d40344-6657-454c-9141-c33e5644506b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d3a2605-7c9b-497a-8699-527233f4efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'dataset.WallSample'>\n",
      "\n",
      "Batch is a named tuple with fields: ('states', 'locations', 'actions')\n",
      "\n",
      "States tensor:\n",
      "- Shape: torch.Size([32, 17, 2, 65, 65])\n",
      "- Type: torch.float32\n",
      "- Device: cuda:0\n",
      "\n",
      "Actions tensor:\n",
      "- Shape: torch.Size([32, 16, 2])\n",
      "- Type: torch.float32\n",
      "- Device: cuda:0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14271b34-840c-40e9-814e-f797194a731f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2752e-43, 2.8403e-39,\n",
       "        3.4870e-35, 2.3690e-31, 8.9062e-28, 1.8529e-24, 2.1331e-21, 1.3590e-18,\n",
       "        4.7909e-16, 9.3466e-14, 1.0090e-11, 6.0282e-10, 1.9929e-08, 3.6459e-07,\n",
       "        3.6910e-06, 2.0678e-05, 6.4104e-05, 1.0997e-04, 1.0440e-04, 5.4847e-05,\n",
       "        1.5945e-05, 2.5651e-06, 2.2836e-07, 1.1250e-08, 3.0669e-10, 4.6267e-12,\n",
       "        3.8624e-14, 1.7843e-16, 4.5615e-19, 6.4530e-22, 5.0517e-25, 2.1884e-28,\n",
       "        5.2463e-32, 6.9598e-36, 5.1091e-40, 2.1019e-44, 0.0000e+00],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b72ec32e-1eef-4797-b416-bec5ee6f0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBE\n",
    "from dataset import create_wall_dataloader\n",
    "from evaluator import ProbingEvaluator\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Check for GPU availability.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    return device\n",
    "\n",
    "\n",
    "def load_data(device):\n",
    "    data_path = \"/scratch/an3854/DL24FA\"\n",
    "\n",
    "    probe_train_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/probe_normal/train\",\n",
    "        probing=True,\n",
    "        device=device,\n",
    "        train=True,\n",
    "    )\n",
    "\n",
    "    probe_val_normal_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/probe_normal/val\",\n",
    "        probing=True,\n",
    "        device=device,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    probe_val_wall_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/probe_wall/val\",\n",
    "        probing=True,\n",
    "        device=device,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    probe_val_ds = {\"normal\": probe_val_normal_ds, \"wall\": probe_val_wall_ds}\n",
    "\n",
    "    return probe_train_ds, probe_val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb62c5f-febb-4243-a399-eb52c2ae6dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aadim)",
   "language": "python",
   "name": "aadim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
