{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict\n",
    "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "from lightly.utils.scheduler import cosine_schedule\n",
    "from dataset import create_wall_dataloader\n",
    "\n",
    "# Configs\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "INPUT_SHAPE = (2, 65, 65)  # 2 channel 65x65 images\n",
    "ACTION_DIM = 2\n",
    "STATE_DIM = 256  # Encoded state dimension\n",
    "HIDDEN_DIM = 600  # GRU hidden dimension\n",
    "BATCH_SIZE = 32\n",
    "MOMENTUM = 0.996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_wall_dataloader(\n",
    "    \"/drive_reader/as16386/DL24FA/train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train=True, \n",
    "    num_samples=None\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'dataset.WallSample'>\n",
      "\n",
      "Batch is a named tuple with fields: ('states', 'locations', 'actions')\n",
      "\n",
      "States tensor:\n",
      "- Shape: torch.Size([32, 17, 2, 65, 65])\n",
      "- Type: torch.float32\n",
      "- Device: cuda:0\n",
      "\n",
      "Actions tensor:\n",
      "- Shape: torch.Size([32, 16, 2])\n",
      "- Type: torch.float32\n",
      "- Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Get one batch from the dataloader\n",
    "train_iter = iter(train_loader)\n",
    "batch = next(train_iter)\n",
    "\n",
    "# Print batch type and contents\n",
    "print(\"Batch type:\", type(batch))\n",
    "print(\"\\nBatch is a named tuple with fields:\", batch._fields)\n",
    "\n",
    "print(\"\\nStates tensor:\")\n",
    "print(\"- Shape:\", batch.states.shape)\n",
    "print(\"- Type:\", batch.states.dtype)\n",
    "print(\"- Device:\", batch.states.device)\n",
    "\n",
    "print(\"\\nActions tensor:\")\n",
    "print(\"- Shape:\", batch.actions.shape)\n",
    "print(\"- Type:\", batch.actions.dtype)\n",
    "print(\"- Device:\", batch.actions.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder parameter count: 1739424\n"
     ]
    }
   ],
   "source": [
    "class StateEncoder(nn.Module):\n",
    "    def __init__(self, state_dim: int = STATE_DIM):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            # 65x65x2 -> 32x32x32\n",
    "            nn.Conv2d(2, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 32x32x32 -> 16x16x64\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 16x16x64 -> 8x8x128\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 8x8x128 -> 4x4x256\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4 * 4 * 256, state_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Test encoder\n",
    "encoder = StateEncoder().to(DEVICE)\n",
    "test_input = torch.randn(BATCH_SIZE, *INPUT_SHAPE).to(DEVICE)\n",
    "test_output = encoder(test_input)\n",
    "assert test_output.shape == (BATCH_SIZE, STATE_DIM), f\"Expected shape {(BATCH_SIZE, STATE_DIM)}, got {test_output.shape}\"\n",
    "print(f\"Encoder parameter count: {sum(p.numel() for p in encoder.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUPredictor(nn.Module):\n",
    "    def __init__(self, state_dim: int = STATE_DIM, action_dim: int = ACTION_DIM, hidden_dim: int = HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_proj = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.action_proj = nn.Linear(action_dim, hidden_dim)\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.out_proj = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, state_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        # Project state\n",
    "        state = self.state_proj(state)  # [B, H]\n",
    "        \n",
    "        # Project action\n",
    "        action = self.action_proj(action)  # [B, H]\n",
    "        \n",
    "        # Prepare for GRU\n",
    "        action = action.unsqueeze(1)  # [B, 1, H]\n",
    "        state = state.unsqueeze(0)    # [1, B, H]\n",
    "        \n",
    "        # GRU forward pass\n",
    "        _, hidden = self.gru(action, state)\n",
    "        output = self.out_proj(hidden[0])\n",
    "        \n",
    "        return output\n",
    "\n",
    "# BYOL projection and prediction heads\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim: int = STATE_DIM):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, input_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "# Test GRU and heads\n",
    "gru = GRUPredictor().to(DEVICE)\n",
    "proj = ProjectionHead().to(DEVICE)\n",
    "pred = PredictionHead().to(DEVICE)\n",
    "\n",
    "test_state = torch.randn(BATCH_SIZE, STATE_DIM).to(DEVICE)\n",
    "test_action = torch.randn(BATCH_SIZE, ACTION_DIM).to(DEVICE)\n",
    "\n",
    "test_gru_out = gru(test_state, test_action)\n",
    "test_proj_out = proj(test_gru_out)\n",
    "test_pred_out = pred(test_proj_out)\n",
    "\n",
    "assert test_gru_out.shape == (BATCH_SIZE, STATE_DIM)\n",
    "assert test_proj_out.shape == (BATCH_SIZE, 256)\n",
    "assert test_pred_out.shape == (BATCH_SIZE, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOLGRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Online networks\n",
    "        self.encoder = StateEncoder()\n",
    "        self.gru = GRUPredictor()\n",
    "        self.projection = ProjectionHead()\n",
    "        self.prediction = PredictionHead()\n",
    "        \n",
    "        # Target networks\n",
    "        self.encoder_momentum = copy.deepcopy(self.encoder)\n",
    "        self.projection_momentum = copy.deepcopy(self.projection)\n",
    "        deactivate_requires_grad(self.encoder_momentum)\n",
    "        deactivate_requires_grad(self.projection_momentum)\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Online forward\n",
    "        z1 = self.encoder(state)\n",
    "        z2 = self.gru(z1, action)\n",
    "        p1 = self.prediction(self.projection(z2))\n",
    "        \n",
    "        # Target forward\n",
    "        with torch.no_grad():\n",
    "            next_state = state  # In our case, we already have the next state\n",
    "            z3 = self.projection_momentum(self.encoder_momentum(next_state))\n",
    "        \n",
    "        return p1, z3\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_target(self, m: float):\n",
    "        \"\"\"Update momentum networks\"\"\"\n",
    "        for online, target in zip(self.encoder.parameters(), self.encoder_momentum.parameters()):\n",
    "            target.data = target.data * m + online.data * (1. - m)\n",
    "        for online, target in zip(self.projection.parameters(), self.projection_momentum.parameters()):\n",
    "            target.data = target.data * m + online.data * (1. - m)\n",
    "\n",
    "# Test full model\n",
    "model = BYOLGRU().to(DEVICE)\n",
    "test_state = torch.randn(BATCH_SIZE, *INPUT_SHAPE).to(DEVICE)\n",
    "test_action = torch.randn(BATCH_SIZE, ACTION_DIM).to(DEVICE)\n",
    "\n",
    "p1, z3 = model(test_state, test_action)\n",
    "assert p1.shape == (BATCH_SIZE, 256)\n",
    "assert z3.shape == (BATCH_SIZE, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 418/4594 [01:50<18:24,  3.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 69\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, epoch, total_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader)):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# states shape: [batch, seq_len, channels, height, width]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# actions shape: [batch, seq_len-1, action_dim]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     states, _, actions \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     14\u001b[0m     batch_size, seq_len \u001b[38;5;241m=\u001b[39m states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/Chill-Pill/dataset.py:32\u001b[0m, in \u001b[0;36mWallDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m---> 32\u001b[0m     states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[i])\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_epoch(model: BYOLGRU, \n",
    "                dataloader: DataLoader, \n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                epoch: int,\n",
    "                total_epochs: int) -> float:\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
    "        # states shape: [batch, seq_len, channels, height, width]\n",
    "        # actions shape: [batch, seq_len-1, action_dim]\n",
    "        states, _, actions = batch\n",
    "        batch_size, seq_len = states.shape[:2]\n",
    "        \n",
    "        # Update momentum parameter\n",
    "        m = cosine_schedule(epoch, total_epochs, 0.996, 1)\n",
    "        \n",
    "        # Process each timestep in the sequence\n",
    "        sequence_loss = 0\n",
    "        for t in range(seq_len - 1):  # -1 because we need next state\n",
    "            current_state = states[:, t]  # [batch, channels, height, width]\n",
    "            next_state = states[:, t+1]   # [batch, channels, height, width]\n",
    "            current_action = actions[:, t] # [batch, action_dim]\n",
    "            \n",
    "            # Forward pass\n",
    "            p1, z3 = model(current_state, current_action)\n",
    "            \n",
    "            # Normalize projections\n",
    "            p1 = F.normalize(p1, dim=-1)\n",
    "            z3 = F.normalize(z3, dim=-1)\n",
    "            \n",
    "            # BYOL loss for this timestep\n",
    "            loss = 2 - 2 * (p1 * z3).sum(dim=-1).mean()\n",
    "            sequence_loss += loss\n",
    "            \n",
    "        # Average loss over sequence\n",
    "        sequence_loss = sequence_loss / (seq_len - 1)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        sequence_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update momentum networks\n",
    "        model.update_target(m)\n",
    "        \n",
    "        total_loss += sequence_loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training setup\n",
    "\n",
    "# print(\"Starting to load data\") \n",
    "# states, actions = load_data(\"/drive_reader/as16386/DL24FA/train\")  # Update path\n",
    "# dataset = TensorDataset(states, actions)\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# print(\"Finished loading data\") \n",
    "# Test shapes from dataloader\n",
    "# test_states, test_actions = next(iter(dataloader))\n",
    "# print(f\"States shape: {test_states.shape}\")\n",
    "# print(f\"Actions shape: {test_actions.shape}\")\n",
    "\n",
    "model = BYOLGRU().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "print(\"Starting training\") \n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_epoch(model, train_loader, optimizer, epoch, num_epochs)\n",
    "    print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, f'byol_try_checkpoints/byol_gru_checkpoint_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_env)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
