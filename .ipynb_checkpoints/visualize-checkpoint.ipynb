{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c18d9cab-f0e2-4e40-aa42-7ba9e53294fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "from models import WorldModelVICReg\n",
    "from dataset import create_wall_dataloader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6610d6dc-8f01-4234-b4d6-8dd42ae8bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNVisualizer:\n",
    "    def __init__(self, model, device='cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        \n",
    "    def _enhance_activation_map(self, activation, method='both', percentile=80):\n",
    "        \"\"\"\n",
    "        Enhance activation map visibility using various methods\n",
    "        Args:\n",
    "            activation: numpy array of activations\n",
    "            method: 'threshold', 'softmax', or 'both'\n",
    "            percentile: percentile value for thresholding\n",
    "        \"\"\"\n",
    "        if method == 'threshold':\n",
    "            threshold = np.percentile(activation, percentile)\n",
    "            enhanced = np.where(activation > threshold, activation, 0)\n",
    "            if enhanced.max() != enhanced.min():\n",
    "                enhanced = (enhanced - enhanced.min()) / (enhanced.max() - enhanced.min())\n",
    "            \n",
    "        elif method == 'softmax':\n",
    "            flattened = activation.flatten()\n",
    "            softmaxed = F.softmax(torch.tensor(flattened) / 0.1, dim=0).numpy()\n",
    "            enhanced = softmaxed.reshape(activation.shape)\n",
    "            \n",
    "        elif method == 'both':\n",
    "            threshold = np.percentile(activation, percentile)\n",
    "            thresholded = np.where(activation > threshold, activation, 0)\n",
    "            flattened = thresholded.flatten()\n",
    "            softmaxed = F.softmax(torch.tensor(flattened) / 0.1, dim=0).numpy()\n",
    "            enhanced = softmaxed.reshape(activation.shape)\n",
    "            \n",
    "        return enhanced\n",
    "    \n",
    "    def visualize_feature_maps(self, input_state, input_action=None, layer_name='conv1', enhance_method='both'):\n",
    "        \"\"\"\n",
    "        Visualize feature maps from a specific convolutional layer\n",
    "        Args:\n",
    "            input_state: Input tensor of shape [B, T, C, H, W] or [B, C, H, W]\n",
    "            input_action: [B, T-1, 2] tensor (optional)\n",
    "            layer_name: name of layer to visualize ('conv1' or 'conv2' for encoder)\n",
    "            enhance_method: method to enhance activation visibility\n",
    "        \"\"\"\n",
    "        # Get a single state to visualize (first timestep if sequence input)\n",
    "        if input_state.dim() == 5:  # [B, T, C, H, W]\n",
    "            input_state = input_state[:, 0]  # Take first timestep -> [B, C, H, W]\n",
    "            \n",
    "        activations = {}\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                activations[name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        if layer_name == 'conv1':\n",
    "            self.model.encoder.conv1.register_forward_hook(get_activation('conv1'))\n",
    "        elif layer_name == 'conv2':\n",
    "            self.model.encoder.conv2.register_forward_hook(get_activation('conv2'))\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            _ = self.model.encoder(input_state)\n",
    "        \n",
    "        feature_maps = activations[layer_name][0].cpu().numpy()  # Take first batch item\n",
    "        n_features = feature_maps.shape[0]\n",
    "        size = int(np.ceil(np.sqrt(n_features)))\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 20))\n",
    "        for i in range(n_features):\n",
    "            enhanced_map = self._enhance_activation_map(feature_maps[i], method=enhance_method)\n",
    "            \n",
    "            ax = plt.subplot(size, size, i + 1)\n",
    "            im = plt.imshow(enhanced_map, cmap='viridis')\n",
    "            plt.colorbar(im)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Filter {i+1}')\n",
    "        \n",
    "        plt.suptitle(f'{layer_name} Feature Maps (Enhanced with {enhance_method})')\n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "    \n",
    "    def visualize_kernel_weights(self, layer_name='conv1', enhance_method='both'):\n",
    "        \"\"\"\n",
    "        Visualize convolutional kernel weights\n",
    "        Args:\n",
    "            layer_name: name of layer to visualize ('conv1' or 'conv2')\n",
    "            enhance_method: method to enhance weight visibility\n",
    "        \"\"\"\n",
    "        if layer_name == 'conv1':\n",
    "            weights = self.model.encoder.conv1.weight.data.cpu()\n",
    "        elif layer_name == 'conv2':\n",
    "            weights = self.model.encoder.conv2.weight.data.cpu()\n",
    "        \n",
    "        out_channels, in_channels, k_h, k_w = weights.shape\n",
    "        \n",
    "        fig, axes = plt.subplots(out_channels, in_channels, \n",
    "                                figsize=(in_channels*3, out_channels*3))\n",
    "        \n",
    "        if out_channels == 1:\n",
    "            axes = axes[np.newaxis, :]\n",
    "        if in_channels == 1:\n",
    "            axes = axes[:, np.newaxis]\n",
    "        \n",
    "        # Plot each kernel\n",
    "        for i in range(out_channels):\n",
    "            for j in range(in_channels):\n",
    "                kernel = weights[i, j].numpy()\n",
    "                enhanced_kernel = self._enhance_activation_map(kernel, method=enhance_method)\n",
    "                \n",
    "                im = axes[i, j].imshow(enhanced_kernel, cmap='viridis')\n",
    "                plt.colorbar(im, ax=axes[i, j])\n",
    "                axes[i, j].axis('off')\n",
    "                if i == 0:\n",
    "                    if j == 0:\n",
    "                        axes[i, j].set_title('Walls Channel')\n",
    "                    elif j == 1:\n",
    "                        axes[i, j].set_title('Agent Channel')\n",
    "        \n",
    "        plt.suptitle(f'{layer_name} Kernels (Enhanced with {enhance_method})\\nRows: Output Channels, Columns: Input Channels')\n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "\n",
    "    def visualize_saliency_maps(self, input_state, input_action):\n",
    "        \"\"\"\n",
    "        Generate saliency maps showing which input pixels are most important\n",
    "        Args:\n",
    "            input_state: [1, 1, 2, H, W] tensor (sequence format)\n",
    "            input_action: [1, T-1, 2] tensor\n",
    "        \"\"\"\n",
    "        gradients = self.compute_activation_gradients(input_state, input_action)\n",
    "        \n",
    "        # Get original input images\n",
    "        input_walls = input_state.squeeze(1)[0, 0].cpu().numpy()\n",
    "        input_agent = input_state.squeeze(1)[0, 1].cpu().numpy()\n",
    "        \n",
    "        # Enhance gradients\n",
    "        walls_gradients = np.abs(gradients[0, 0])\n",
    "        agent_gradients = np.abs(gradients[0, 1])\n",
    "        enhanced_walls = self._enhance_activation_map(walls_gradients, method='both')\n",
    "        enhanced_agent = self._enhance_activation_map(agent_gradients, method='both')\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Plot original inputs\n",
    "        axes[0, 0].imshow(input_walls, cmap='gray')\n",
    "        axes[0, 0].set_title('Original Walls/Doors')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].imshow(input_agent, cmap='gray')\n",
    "        axes[0, 1].set_title('Original Agent Position')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # Plot enhanced saliency maps\n",
    "        walls_map = axes[1, 0].imshow(enhanced_walls, cmap='hot')\n",
    "        axes[1, 0].set_title('Walls Channel Saliency\\n(Brighter = More Important)')\n",
    "        axes[1, 0].axis('off')\n",
    "        plt.colorbar(walls_map, ax=axes[1, 0])\n",
    "        \n",
    "        agent_map = axes[1, 1].imshow(enhanced_agent, cmap='hot')\n",
    "        axes[1, 1].set_title('Agent Channel Saliency\\n(Brighter = More Important)')\n",
    "        axes[1, 1].axis('off')\n",
    "        plt.colorbar(agent_map, ax=axes[1, 1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "    \n",
    "    def compute_activation_gradients(self, input_state, input_action, target_layer='conv2'):\n",
    "        \"\"\"Helper method for saliency maps\"\"\"\n",
    "        input_state = input_state.clone()\n",
    "        input_state.requires_grad_()\n",
    "        \n",
    "        x = input_state.squeeze(1)\n",
    "        \n",
    "        if target_layer == 'conv1':\n",
    "            activations = self.model.encoder.conv1(x)\n",
    "        elif target_layer == 'conv2':\n",
    "            x = F.relu(self.model.encoder.bn1(self.model.encoder.conv1(x)))\n",
    "            activations = self.model.encoder.conv2(x)\n",
    "            \n",
    "        activation_mean = activations.mean()\n",
    "        activation_mean.backward()\n",
    "        \n",
    "        return input_state.grad.squeeze(1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f90ab03-3533-45ae-80ea-915aa7dcb8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1178448/2546645538.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating encoder conv1 feature maps...\n",
      "Generating encoder conv2 feature maps...\n",
      "Generating saliency maps...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 2, 2, 65, 65]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[43mvisualize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 67\u001b[0m, in \u001b[0;36mvisualize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# 3. Generate saliency maps\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating saliency maps...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m saliency_maps \u001b[38;5;241m=\u001b[39m \u001b[43mvisualizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_saliency_maps\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m saliency_maps\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaliency_maps.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[25], line 128\u001b[0m, in \u001b[0;36mCNNVisualizer.visualize_saliency_maps\u001b[0;34m(self, input_state, input_action)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_saliency_maps\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_state, input_action):\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    Generate saliency maps showing which input pixels are most important\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m        input_state: [1, 1, 2, H, W] tensor (sequence format)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m        input_action: [1, T-1, 2] tensor\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_activation_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Get original input images\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     input_walls \u001b[38;5;241m=\u001b[39m input_state\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[25], line 175\u001b[0m, in \u001b[0;36mCNNVisualizer.compute_activation_gradients\u001b[0;34m(self, input_state, input_action, target_layer)\u001b[0m\n\u001b[1;32m    173\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m target_layer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 175\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    176\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[1;32m    178\u001b[0m activation_mean \u001b[38;5;241m=\u001b[39m activations\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1795\u001b[0m     ):\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/drive_reader/as16386/DL-final-proj/dl_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 2, 2, 65, 65]"
     ]
    }
   ],
   "source": [
    "def load_trained_model(checkpoint_path, loss_type='vicreg'):\n",
    "    \"\"\"\n",
    "    Load a trained WorldModelVICReg from a checkpoint\n",
    "    \"\"\"\n",
    "    # Initialize model with same parameters as training\n",
    "    model = WorldModelVICReg(\n",
    "        loss_type=loss_type,\n",
    "        lambda_param=25.0 if loss_type == 'vicreg' else 0.005,\n",
    "        mu_param=25.0 if loss_type == 'vicreg' else None,\n",
    "        nu_param=1.0 if loss_type == 'vicreg' else None\n",
    "    )\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_sample_batch(data_path=\"/drive_reader/as16386/DL24FA/train\", batch_size=1):\n",
    "    \"\"\"\n",
    "    Get a sample batch from the dataset\n",
    "    \"\"\"\n",
    "    loader = create_wall_dataloader(\n",
    "        data_path=data_path,\n",
    "        probing=False,\n",
    "        device=\"cuda\",\n",
    "        batch_size=batch_size,\n",
    "        train=False,\n",
    "        num_samples=batch_size\n",
    "    )\n",
    "    return next(iter(loader))\n",
    "\n",
    "def visualize_model():\n",
    "    # Set parameters\n",
    "    checkpoint_dir = \"checkpoints/vicreg_high_lr_cosine\"  # Replace with your wandb run name\n",
    "    # latest_checkpoint = max([f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint')],\n",
    "    #                       key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    latest_checkpoint = 'checkpoint_epoch_1.pt' \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "    # checkpoint_path = '/drive_reader/as16386/DL-final-proj/Chill-Pill/checkpoints/vicreg_long_training_onecycle/checkpoint_epoch_30.pt'\n",
    "    # Load model\n",
    "    model = load_trained_model(checkpoint_path, loss_type='vicreg')  # or 'barlow' depending on your training\n",
    "    model = model.cuda()\n",
    "    \n",
    "    # Get a sample batch\n",
    "    batch = get_sample_batch()\n",
    "    input_state = batch.states[:, :2].cuda()  # Get just the first state\n",
    "    input_action = batch.actions.cuda()\n",
    "    \n",
    "    # Initialize visualizer\n",
    "    visualizer = CNNVisualizer(model)\n",
    "    \n",
    "    # 1. Visualize encoder's first conv layer feature maps\n",
    "    print(\"Generating encoder conv1 feature maps...\")\n",
    "    encoder_conv1_maps = visualizer.visualize_feature_maps(input_state[:,0], layer_name='conv1')\n",
    "    encoder_conv1_maps.savefig('encoder_conv1_features.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Visualize encoder's second conv layer feature maps\n",
    "    print(\"Generating encoder conv2 feature maps...\")\n",
    "    encoder_conv2_maps = visualizer.visualize_feature_maps(input_state[:,0], layer_name='conv2')\n",
    "    encoder_conv2_maps.savefig('encoder_conv2_features.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Generate saliency maps\n",
    "    print(\"Generating saliency maps...\")\n",
    "    saliency_maps = visualizer.visualize_saliency_maps(input_state, input_action)\n",
    "    saliency_maps.savefig('saliency_maps.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Visualize convolutional kernels\n",
    "    print(\"Generating kernel visualizations...\")\n",
    "    conv1_kernels = visualizer.visualize_kernel_weights(layer_name='conv1')\n",
    "    conv1_kernels.savefig('conv1_kernels.png')\n",
    "    plt.close()\n",
    "    \n",
    "    conv2_kernels = visualizer.visualize_kernel_weights(layer_name='conv2')\n",
    "    conv2_kernels.savefig('conv2_kernels.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a6f4e-15e7-4265-aad7-08b729487624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorVisualizer:\n",
    "    def __init__(self, model, device='cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "    \n",
    "    def _enhance_activation_map(self, activation, method='threshold', percentile=80):\n",
    "        \"\"\"\n",
    "        Enhance activation map visibility using various methods\n",
    "        Args:\n",
    "            activation: numpy array of activations\n",
    "            method: 'threshold', 'softmax', or 'both'\n",
    "            percentile: percentile value for thresholding\n",
    "        \"\"\"\n",
    "        if method == 'threshold':\n",
    "            # Zero out values below threshold\n",
    "            threshold = np.percentile(activation, percentile)\n",
    "            enhanced = np.where(activation > threshold, activation, 0)\n",
    "            # Normalize to [0, 1]\n",
    "            if enhanced.max() != enhanced.min():\n",
    "                enhanced = (enhanced - enhanced.min()) / (enhanced.max() - enhanced.min())\n",
    "            \n",
    "        elif method == 'softmax':\n",
    "            # Apply softmax to create more separation\n",
    "            flattened = activation.flatten()\n",
    "            softmaxed = F.softmax(torch.tensor(flattened) / 0.1, dim=0).numpy()\n",
    "            enhanced = softmaxed.reshape(activation.shape)\n",
    "            \n",
    "        elif method == 'both':\n",
    "            # Combine both methods\n",
    "            threshold = np.percentile(activation, percentile)\n",
    "            thresholded = np.where(activation > threshold, activation, 0)\n",
    "            flattened = thresholded.flatten()\n",
    "            softmaxed = F.softmax(torch.tensor(flattened) / 0.1, dim=0).numpy()\n",
    "            enhanced = softmaxed.reshape(activation.shape)\n",
    "            \n",
    "        return enhanced\n",
    "    \n",
    "    def visualize_prediction_sequence(self, input_state, actions, num_steps=5, enhance_method='both'):\n",
    "        \"\"\"\n",
    "        Visualize how the predictor transforms states over multiple timesteps\n",
    "        Args:\n",
    "            input_state: [1, 1, 2, H, W] initial state\n",
    "            actions: [1, T, 2] sequence of actions\n",
    "            num_steps: number of steps to visualize\n",
    "            enhance_method: method to enhance activation visibility\n",
    "        \"\"\"\n",
    "        num_steps = min(num_steps, actions.shape[1])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = self.model.forward_prediction(input_state, actions)\n",
    "            \n",
    "            fig, axes = plt.subplots(num_steps, 4, figsize=(20, 4*num_steps))\n",
    "            \n",
    "            # For t=0, show the initial state\n",
    "            axes[0, 0].imshow(input_state.squeeze(1)[0, 0].cpu().numpy(), cmap='gray')\n",
    "            axes[0, 0].set_title('Initial Walls/Doors')\n",
    "            axes[0, 0].axis('off')\n",
    "            \n",
    "            axes[0, 1].imshow(input_state.squeeze(1)[0, 1].cpu().numpy(), cmap='gray')\n",
    "            axes[0, 1].set_title('Initial Agent Position')\n",
    "            axes[0, 1].axis('off')\n",
    "            \n",
    "            action = actions[0, 0].cpu().numpy()\n",
    "            axes[0, 2].text(0.5, 0.5, f'Action: [{action[0]:.2f}, {action[1]:.2f}]', \n",
    "                          horizontalalignment='center')\n",
    "            axes[0, 2].set_title('Action Applied')\n",
    "            axes[0, 2].axis('off')\n",
    "            \n",
    "            axes[0, 3].text(0.5, 0.5, 'Initial State', horizontalalignment='center')\n",
    "            axes[0, 3].axis('off')\n",
    "            \n",
    "            # For each subsequent timestep\n",
    "            for t in range(1, num_steps):\n",
    "                pred_encoding = predictions[:, t]  # Shape: [1, 32, 8, 8]\n",
    "                \n",
    "                # Enhance and plot wall features\n",
    "                wall_encoding = pred_encoding[:, :16].mean(dim=1).squeeze(0).cpu().numpy()\n",
    "                enhanced_wall = self._enhance_activation_map(wall_encoding, method=enhance_method)\n",
    "                wall_im = axes[t, 0].imshow(enhanced_wall, cmap='viridis')\n",
    "                axes[t, 0].set_title(f'T{t} Wall Features')\n",
    "                axes[t, 0].axis('off')\n",
    "                plt.colorbar(wall_im, ax=axes[t, 0])\n",
    "                \n",
    "                # Enhance and plot agent features\n",
    "                agent_encoding = pred_encoding[:, 16:].mean(dim=1).squeeze(0).cpu().numpy()\n",
    "                enhanced_agent = self._enhance_activation_map(agent_encoding, method=enhance_method)\n",
    "                agent_im = axes[t, 1].imshow(enhanced_agent, cmap='viridis')\n",
    "                axes[t, 1].set_title(f'T{t} Agent Features')\n",
    "                axes[t, 1].axis('off')\n",
    "                plt.colorbar(agent_im, ax=axes[t, 1])\n",
    "                \n",
    "                # Show action\n",
    "                action = actions[0, t-1].cpu().numpy()\n",
    "                axes[t, 2].text(0.5, 0.5, f'Action: [{action[0]:.2f}, {action[1]:.2f}]', \n",
    "                              horizontalalignment='center')\n",
    "                axes[t, 2].set_title('Action Applied')\n",
    "                axes[t, 2].axis('off')\n",
    "                \n",
    "                # Enhance and plot mean activation\n",
    "                mean_activation = pred_encoding.mean(dim=1).squeeze(0).cpu().numpy()\n",
    "                enhanced_mean = self._enhance_activation_map(mean_activation, method=enhance_method)\n",
    "                mean_im = axes[t, 3].imshow(enhanced_mean, cmap='viridis')\n",
    "                axes[t, 3].set_title(f'T{t} Mean Activation')\n",
    "                axes[t, 3].axis('off')\n",
    "                plt.colorbar(mean_im, ax=axes[t, 3])\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            return plt.gcf()\n",
    "    \n",
    "    def visualize_action_embeddings(self, input_state, actions, enhance_method='both'):\n",
    "        \"\"\"\n",
    "        Visualize how the predictor embeds actions into the spatial domain\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            B, _, H, W = input_state.squeeze(1).shape\n",
    "            action = actions[:, 0]\n",
    "            action_spatial = action.view(B, 2, 1, 1).expand(-1, -1, H, W)\n",
    "            action_embedding = self.model.predictor.action_embed(action_spatial)\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "            for i in range(16):\n",
    "                row = i // 8\n",
    "                col = i % 8\n",
    "                embedding = action_embedding[0, i].cpu().numpy()\n",
    "                enhanced = self._enhance_activation_map(embedding, method=enhance_method)\n",
    "                im = axes[row, col].imshow(enhanced, cmap='viridis')\n",
    "                axes[row, col].set_title(f'Channel {i+1}')\n",
    "                axes[row, col].axis('off')\n",
    "                plt.colorbar(im, ax=axes[row, col])\n",
    "            \n",
    "            plt.suptitle(f'Action Embedding Channels for action: [{action[0,0]:.2f}, {action[0,1]:.2f}]')\n",
    "            plt.tight_layout()\n",
    "            return plt.gcf()\n",
    "    \n",
    "    def visualize_transition_layers(self, input_state, actions, enhance_method='both'):\n",
    "        \"\"\"\n",
    "        Visualize activations in the transition model layers\n",
    "        \"\"\"\n",
    "        activations = {}\n",
    "        \n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                activations[name] = output.detach()\n",
    "            return hook\n",
    "        \n",
    "        handles = []\n",
    "        for name, module in self.model.predictor.transition.named_children():\n",
    "            if isinstance(module, torch.nn.Conv2d):\n",
    "                handles.append(module.register_forward_hook(get_activation(name)))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = self.model.forward_prediction(input_state, actions)\n",
    "        \n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        for idx, (name, activation) in enumerate(activations.items()):\n",
    "            mean_activation = activation[0].mean(dim=0).cpu().numpy()\n",
    "            enhanced = self._enhance_activation_map(mean_activation, method=enhance_method)\n",
    "            \n",
    "            plt.subplot(1, len(activations), idx+1)\n",
    "            im = plt.imshow(enhanced, cmap='viridis')\n",
    "            plt.title(f'Transition Layer {idx+1}\\nMean Activation')\n",
    "            plt.colorbar(im)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f8b00b-f4e2-42bd-90bb-c8ad672f495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"checkpoints/vicreg_high_lr_cosine\"  # Replace with your wandb run name\n",
    "latest_checkpoint = 'checkpoint_epoch_1.pt' \n",
    "checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "\n",
    "# Load model\n",
    "model = load_trained_model(checkpoint_path, loss_type='vicreg')  # or 'barlow' depending on your training\n",
    "model = model.cuda()\n",
    "\n",
    "# Get a sample batch\n",
    "batch = get_sample_batch()\n",
    "input_state = batch.states[:, 0:1].cuda()  # Get just the first state\n",
    "input_action = batch.actions.cuda()\n",
    "\n",
    "# Create visualizer\n",
    "predictor_viz = PredictorVisualizer(model)\n",
    "\n",
    "# 1. Visualize prediction sequence\n",
    "sequence_viz = predictor_viz.visualize_prediction_sequence(input_state, input_action, num_steps=5)\n",
    "plt.show()\n",
    "\n",
    "# 2. Visualize action embeddings\n",
    "action_viz = predictor_viz.visualize_action_embeddings(input_state, input_action)\n",
    "plt.show()\n",
    "\n",
    "# 3. Visualize transition layer activations\n",
    "transition_viz = predictor_viz.visualize_transition_layers(input_state, input_action)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c6f41-4b14-43f9-8528-2148e58d420e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_env)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
