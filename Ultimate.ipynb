{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91288f36-61a1-4056-9040-62ba4daed064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4594 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 241\u001b[0m\n\u001b[1;32m    239\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m NUM_EPOCHS\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 241\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Optionally save the model after training\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 217\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, epoch, total_epochs)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[1;32m    216\u001b[0m     cs \u001b[38;5;241m=\u001b[39m current_states[:, t]  \u001b[38;5;66;03m# [B, C, H, W]\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     ns \u001b[38;5;241m=\u001b[39m \u001b[43mnext_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m     \u001b[38;5;66;03m# [B, C, H, W]\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     a \u001b[38;5;241m=\u001b[39m actions[:, t]          \u001b[38;5;66;03m# [B, action_dim]\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     p1, z3 \u001b[38;5;241m=\u001b[39m model(cs, ns, a)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from lightly.models.utils import deactivate_requires_grad\n",
    "from lightly.utils.scheduler import cosine_schedule\n",
    "from dataset import create_wall_dataloader\n",
    "\n",
    "# Configurations\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 32\n",
    "STATE_CHANNELS = 2\n",
    "STATE_H = 65\n",
    "STATE_W = 65\n",
    "ACTION_DIM = 2\n",
    "SPATIAL_DIM = 8        # Spatial dimension for latent features\n",
    "OUTPUT_CHANNELS = 64    # Output channels for latent representation\n",
    "PROJ_DIM = 256\n",
    "INIT_MOMENTUM = 0.996\n",
    "LR = 3e-4\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Dataloader creation (assuming create_wall_dataloader returns a loader with (current_states, next_states, actions)\n",
    "train_loader = create_wall_dataloader(\n",
    "    \"/scratch/an3854/DL24FA/train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train=True, \n",
    ")\n",
    "\n",
    "class SpatialStateEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes an input state into a spatial feature map of size [C, spatial_dim, spatial_dim].\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=2, output_channels=64, spatial_dim=8):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # After these layers: 65x65 -> ~4x4\n",
    "        # Map 4x4x256 -> 4x4x64 and then upsample to 8x8 if needed\n",
    "        self.spatial_predictor = nn.Sequential(\n",
    "            nn.Conv2d(256, output_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(size=(spatial_dim, spatial_dim), mode='bilinear', align_corners=False) \n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)  \n",
    "        x = self.spatial_predictor(x)  \n",
    "        return x\n",
    "\n",
    "class ConvDetTransition(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional deterministic transition model:\n",
    "    Given current spatial latent and action, predicts next spatial latent.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_channels=64, action_dim=2, hidden_channels=128):\n",
    "        super().__init__()\n",
    "        self.action_proj = nn.Sequential(\n",
    "            nn.Linear(action_dim, hidden_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transition = nn.Sequential(\n",
    "            nn.Conv2d(state_channels + hidden_channels, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, state_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = state.shape\n",
    "        action_feat = self.action_proj(action)  # [B, hidden_channels]\n",
    "        action_feat = action_feat.unsqueeze(-1).unsqueeze(-1)  # [B, hidden_channels, 1, 1]\n",
    "        action_feat = action_feat.expand(B, action_feat.shape[1], H, W)  # broadcast over spatial dims\n",
    "        x = torch.cat([state, action_feat], dim=1)\n",
    "        next_state = self.transition(x)\n",
    "        return next_state\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    BYOL projection head:\n",
    "    Flattens spatial representation and projects to a lower-dimensional space.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=64, spatial_dim=8, proj_dim=256):\n",
    "        super().__init__()\n",
    "        input_dim = input_channels * spatial_dim * spatial_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, proj_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.flatten(start_dim=1)  # [B, C*H*W]\n",
    "        return self.net(x)\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    BYOL prediction head:\n",
    "    Takes the projection and predicts a representation closer to the target projection.\n",
    "    \"\"\"\n",
    "    def __init__(self, proj_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(proj_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, proj_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class BYOLConvDet(nn.Module):\n",
    "    \"\"\"\n",
    "    BYOL model using a convolutional deterministic state transition.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 state_channels=STATE_CHANNELS, \n",
    "                 spatial_dim=SPATIAL_DIM, \n",
    "                 output_channels=OUTPUT_CHANNELS, \n",
    "                 action_dim=ACTION_DIM, \n",
    "                 proj_dim=PROJ_DIM,\n",
    "                 momentum=INIT_MOMENTUM):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Online networks\n",
    "        self.online_encoder = SpatialStateEncoder(input_channels=state_channels, output_channels=output_channels, spatial_dim=spatial_dim)\n",
    "        self.online_transition = ConvDetTransition(state_channels=output_channels, action_dim=action_dim)\n",
    "        self.online_projection = ProjectionHead(input_channels=output_channels, spatial_dim=spatial_dim, proj_dim=proj_dim)\n",
    "        self.online_prediction = PredictionHead(proj_dim=proj_dim)\n",
    "        \n",
    "        # Target networks\n",
    "        self.target_encoder = copy.deepcopy(self.online_encoder)\n",
    "        self.target_projection = copy.deepcopy(self.online_projection)\n",
    "        deactivate_requires_grad(self.target_encoder)\n",
    "        deactivate_requires_grad(self.target_projection)\n",
    "        \n",
    "        self.momentum = momentum\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_target(self, m: float):\n",
    "        \"\"\"Momentum update for target networks.\"\"\"\n",
    "        for o, t in zip(self.online_encoder.parameters(), self.target_encoder.parameters()):\n",
    "            t.data = t.data * m + o.data * (1. - m)\n",
    "        for o, t in zip(self.online_projection.parameters(), self.target_projection.parameters()):\n",
    "            t.data = t.data * m + o.data * (1. - m)\n",
    "\n",
    "    def forward(self, current_state: torch.Tensor, next_state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Encode current state\n",
    "        z1 = self.online_encoder(current_state)  # [B, C, H, W]\n",
    "\n",
    "        # Predict next latent\n",
    "        z2_pred_spatial = self.online_transition(z1, action)  # [B, C, H, W]\n",
    "\n",
    "        # Online pipeline\n",
    "        z2_pred_proj = self.online_projection(z2_pred_spatial)  # [B, proj_dim]\n",
    "        p1 = self.online_prediction(z2_pred_proj)               # [B, proj_dim]\n",
    "\n",
    "        # Target pipeline (no grad)\n",
    "        with torch.no_grad():\n",
    "            z3_spatial = self.target_encoder(next_state)\n",
    "            z3_proj = self.target_projection(z3_spatial)  # [B, proj_dim]\n",
    "        \n",
    "        return p1, z3_proj\n",
    "\n",
    "def byol_loss(p: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "    p = F.normalize(p, dim=-1)\n",
    "    z = F.normalize(z, dim=-1)\n",
    "    return 2 - 2 * (p * z).sum(dim=-1).mean()\n",
    "\n",
    "model = BYOLConvDet().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def train_epoch(model: BYOLConvDet, \n",
    "                dataloader: DataLoader, \n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                epoch: int,\n",
    "                total_epochs: int) -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
    "        # batch = (current_states, next_states, actions)\n",
    "        # current_states: [B, T, C, H, W]\n",
    "        # next_states:    [B, T, C, H, W]\n",
    "        # actions:        [B, T, action_dim]\n",
    "        current_states, next_states, actions = batch\n",
    "        current_states = current_states.to(DEVICE)\n",
    "        next_states = next_states.to(DEVICE)\n",
    "        actions = actions.to(DEVICE)\n",
    "        \n",
    "        # Compute current momentum\n",
    "        m = cosine_schedule(epoch, total_epochs, INIT_MOMENTUM, 1.0)\n",
    "        \n",
    "        # We now loop over all time steps T and sum the losses.\n",
    "        B, T, C, H, W = current_states.shape\n",
    "        step_loss = 0.0\n",
    "        for t in range(T):\n",
    "            cs = current_states[:, t]  # [B, C, H, W]\n",
    "            ns = next_states[:, t]     # [B, C, H, W]\n",
    "            a = actions[:, t]          # [B, action_dim]\n",
    "            \n",
    "            p1, z3 = model(cs, ns, a)\n",
    "            l = byol_loss(p1, z3)\n",
    "            step_loss += l\n",
    "        \n",
    "        # Average loss over T steps\n",
    "        step_loss = step_loss / T\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        step_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update target networks\n",
    "        model.update_target(m)\n",
    "        \n",
    "        total_loss += step_loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = NUM_EPOCHS\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_epoch(model, train_loader, optimizer, epoch, num_epochs)\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Optionally save the model after training\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "print(\"Training complete. model_weights.pth saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d40344-6657-454c-9141-c33e5644506b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d3a2605-7c9b-497a-8699-527233f4efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch type: <class 'dataset.WallSample'>\n",
      "\n",
      "Batch is a named tuple with fields: ('states', 'locations', 'actions')\n",
      "\n",
      "States tensor:\n",
      "- Shape: torch.Size([32, 17, 2, 65, 65])\n",
      "- Type: torch.float32\n",
      "- Device: cuda:0\n",
      "\n",
      "Actions tensor:\n",
      "- Shape: torch.Size([32, 16, 2])\n",
      "- Type: torch.float32\n",
      "- Device: cuda:0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14271b34-840c-40e9-814e-f797194a731f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2752e-43, 2.8403e-39,\n",
       "        3.4870e-35, 2.3690e-31, 8.9062e-28, 1.8529e-24, 2.1331e-21, 1.3590e-18,\n",
       "        4.7909e-16, 9.3466e-14, 1.0090e-11, 6.0282e-10, 1.9929e-08, 3.6459e-07,\n",
       "        3.6910e-06, 2.0678e-05, 6.4104e-05, 1.0997e-04, 1.0440e-04, 5.4847e-05,\n",
       "        1.5945e-05, 2.5651e-06, 2.2836e-07, 1.1250e-08, 3.0669e-10, 4.6267e-12,\n",
       "        3.8624e-14, 1.7843e-16, 4.5615e-19, 6.4530e-22, 5.0517e-25, 2.1884e-28,\n",
       "        5.2463e-32, 6.9598e-36, 5.1091e-40, 2.1019e-44, 0.0000e+00],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b72ec32e-1eef-4797-b416-bec5ee6f0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBE\n",
    "from dataset import create_wall_dataloader\n",
    "from evaluator import ProbingEvaluator\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Check for GPU availability.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    return device\n",
    "\n",
    "\n",
    "def load_data(device):\n",
    "    data_path = \"/scratch/an3854/DL24FA\"\n",
    "\n",
    "    probe_train_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/probe_normal/train\",\n",
    "        probing=True,\n",
    "        device=device,\n",
    "        train=True,\n",
    "    )\n",
    "\n",
    "    probe_val_normal_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/probe_normal/val\",\n",
    "        probing=True,\n",
    "        device=device,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    probe_val_wall_ds = create_wall_dataloader(\n",
    "        data_path=f\"{data_path}/probe_wall/val\",\n",
    "        probing=True,\n",
    "        device=device,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    probe_val_ds = {\"normal\": probe_val_normal_ds, \"wall\": probe_val_wall_ds}\n",
    "\n",
    "    return probe_train_ds, probe_val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cb62c5f-febb-4243-a399-eb52c2ae6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(device, model, probe_train_ds, probe_val_ds):\n",
    "    evaluator = ProbingEvaluator(\n",
    "        device=device,\n",
    "        model=model,\n",
    "        probe_train_ds=probe_train_ds,\n",
    "        probe_val_ds=probe_val_ds,\n",
    "        quick_debug=False,\n",
    "    )\n",
    "\n",
    "    prober = evaluator.train_pred_prober()\n",
    "\n",
    "    avg_losses = evaluator.evaluate_all(prober=prober)\n",
    "\n",
    "    for probe_attr, loss in avg_losses.items():\n",
    "        print(f\"{probe_attr} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1ad6d10-0c1d-404f-b281-e855c0fffcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "probe_train_ds, probe_val_ds = load_data(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5c97700-047e-46e2-8903-f2f3463e8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Does nothing. Just for testing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device=\"cuda\", bs=64, n_steps=17, output_dim=256):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.bs = bs\n",
    "        self.n_steps = n_steps\n",
    "        self.repr_dim = 256\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            states: [B, 1, Ch, H, W]\n",
    "            actions: [B, T-1, 2]\n",
    "\n",
    "        Output:\n",
    "            predictions: [B, T, D]\n",
    "        \"\"\"\n",
    "        return torch.randn((self.bs, self.n_steps, self.repr_dim)).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf38bba9-c61d-45de-9a90-a5433a303615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b13d7c6aa749a99fbc4535a2d6ca0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Probe prediction epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859b601f33d945f4b23ad561179330c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Probe prediction step:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized pred locations loss 1.1928168535232544\n",
      "normalized pred locations loss 1.043358564376831\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd170e703c8c4749a24cf66a857438ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Probe prediction step:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized pred locations loss 0.8633273839950562\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MockModel()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobe_train_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobe_val_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 10\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(device, model, probe_train_ds, probe_val_ds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(device, model, probe_train_ds, probe_val_ds):\n\u001b[1;32m      2\u001b[0m     evaluator \u001b[38;5;241m=\u001b[39m ProbingEvaluator(\n\u001b[1;32m      3\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m      4\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m         quick_debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0m     prober \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_pred_prober\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     avg_losses \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate_all(prober\u001b[38;5;241m=\u001b[39mprober)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m probe_attr, loss \u001b[38;5;129;01min\u001b[39;00m avg_losses\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/scratch/an3854/Chill-Pill/evaluator.py:112\u001b[0m, in \u001b[0;36mProbingEvaluator.train_pred_prober\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m Scheduler(\n\u001b[1;32m    102\u001b[0m     schedule\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mschedule,\n\u001b[1;32m    103\u001b[0m     base_lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbe prediction epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbe prediction step\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;66;03m# TODO: Forward pass through your model\u001b[39;00m\n\u001b[1;32m    115\u001b[0m         pred_encs \u001b[38;5;241m=\u001b[39m model(states\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mstates, actions\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mactions)\n\u001b[1;32m    116\u001b[0m         pred_encs \u001b[38;5;241m=\u001b[39m pred_encs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# # BS, T, D --> T, BS, D\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/an3854/aadim/lib64/python3.9/site-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/an3854/aadim/lib64/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/an3854/aadim/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/scratch/an3854/aadim/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/an3854/aadim/lib64/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/an3854/aadim/lib64/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/an3854/Chill-Pill/dataset.py:32\u001b[0m, in \u001b[0;36mWallDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m---> 32\u001b[0m     states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[i])\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MockModel().to(device)\n",
    "evaluate_model(device, model, probe_train_ds, probe_val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40d54a-78ef-4840-9214-61c422e9411f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aadim)",
   "language": "python",
   "name": "aadim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
